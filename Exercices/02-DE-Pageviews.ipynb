{"cells":[{"cell_type":"markdown","source":["#![Spark Logo](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark.png)\n\n**Objective:**\nAnalyze Desktop vs Mobile traffic to English Wikipedia\n\n**Time to Complete:**\n30 mins\n\n**Data Source:**\npageviews_by_second (<a href=\"http://datahub.io/en/dataset/english-wikipedia-pageviews-by-second\" target=\"_blank\">255 MB</a>)\n\n**Business Questions:**\n* Question # 1) How many rows in the table refer to mobile vs desktop?\n\n**Engineering Questions:**\n* How is the data partitioned? Why is it partitioned the way it is?\n\n**Technical Accomplishments:**\n- Upload a file to Databricks using the Tables UI (optional)\n- Learn how Actions kick off Jobs + Stages\n- Understand how DataFrame partitions relate to compute tasks\n- Use Spark UI to monitor details of Job execution (input read, Shuffle, Storage UI, SQL visualization)\n- Cache a DataFrame to memory (and learn how to unpersist it)\n- Use the following transformations: `orderBy()`, `filter()`\n- Catalyst Optimizer: How DataFrame queries are converted from a Logical plan -> Physical plan\n- Configuration Option: `spark.sql.shuffle.partitions`\n\n**NOTE** Please run this notebook in a Spark 2.0 cluster."],"metadata":{}},{"cell_type":"markdown","source":["Attach to, and then restart your cluster first to clear out old memory caches and get to a default, standard environment. The restart should take 1 - 2 minutes.\n\n![Restart Cluster](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/restart_cluster.png)"],"metadata":{}},{"cell_type":"markdown","source":["####![Wikipedia Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_wikipedia_tiny.png) **Introduction: Pageviews By Second**"],"metadata":{}},{"cell_type":"markdown","source":["Wikipedia.com is the 7th most popular website (measured by page views and unique visitors):"],"metadata":{}},{"cell_type":"markdown","source":["#![Top Ten Global Websites2](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/top_ten_websites.png)\n\nSource: Alexa/Aug 2015: <a href=\"https://en.wikipedia.org/wiki/List_of_most_popular_websites\" target=\"_blank\">List_of_most_popular_websites</a>"],"metadata":{}},{"cell_type":"markdown","source":["In this notebook, we will analyze the traffic patterns to the desktop vs. mobile editions of English Wikipedia.\n\nThe Wikimedia Foundation has released 41 days of pageviews data starting March 16, 2015 at midnight. Two rows are collected every second:\n- Desktop requests\n- Mobile requests"],"metadata":{}},{"cell_type":"markdown","source":["####![Databricks Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_databricks_tiny.png) ** Let's use a premounted version of the data (from S3)**\nLet's start by taking another look at what is on our file system: run the following cell and we should see all the datasets from Amazon S3 mounted into our shard:"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/wikipedia-readonly/\"))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Next, let's take a look in our *pageviews* folder:"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/wikipedia-readonly/pageviews\"))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["We can import this file directly with the following command:"],"metadata":{}},{"cell_type":"code","source":["temp_df = (spark.read\n   .option(\"header\", \"true\")        # Use first line of all files as header\n   .option(\"inferSchema\", \"true\")   # Automatically infer data types\n   .option(\"delimiter\", \"\\t\")       # Use tab delimiter (default is comma-separator)\n   .csv(\"/mnt/wikipedia-readonly/pageviews/pageviews_by_second.tsv\")\n)\ntemp_df.createOrReplaceTempView(\"pageviews_by_second\")\n\n# Note: In versions of Spark prior to 2.0, do this, instead:\n#temp_df = (sqlContext.read\n#   .format(\"com.databricks.spark.csv\")\n#   .option(\"header\", \"true\")        # Use first line of all files as header\n#   .option(\"inferSchema\", \"true\")   # Automatically infer data types\n#   .option(\"delimiter\", \"\\t\")       # Use tab delimiter (default is comma-separator)\n#   .load(\"/mnt/wikipedia-readonly/pageviews/pageviews_by_second.tsv\")\n#)\n#temp_df.registerTempTable(\"pageviews_by_second\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Lastly, we can verify that the \"table\" exists by using `spark` to create the `pageviewsDF` from the \"temp\" table \"pageviews_by_second\""],"metadata":{}},{"cell_type":"code","source":["pageviews_df = spark.read.table(\"pageviews_by_second\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["And then we can take a look at the first 10 records."],"metadata":{}},{"cell_type":"code","source":["pageviews_df.show(10)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Next, take note that the timestamps and/or sites are out of order. We will dig into this more later."],"metadata":{}},{"cell_type":"markdown","source":["Click the down arrows in the cell above to see that the `show()` action kicked off 1 job and 1 stage.\n\nWe will learn more about Jobs and Stages later in this lab."],"metadata":{}},{"cell_type":"markdown","source":["`printSchema()` prints out the schema for the table, the data types for each column and whether a column can be null:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Notice above that the first 2 columns are typed as `string`, while the requests column holds an `integer`."],"metadata":{}},{"cell_type":"markdown","source":["####![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) **Partitions and Tasks**"],"metadata":{}},{"cell_type":"markdown","source":["DataFrames are made of one or more partitions.  To see the number of partitions a DataFrame is made of:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Why? Let's see how many \"cores\" we have in this cluster:"],"metadata":{}},{"cell_type":"markdown","source":["The above cell first converts the DataFrame to an RDD, then calls the partitions method followed by the size method on the RDD. We will learn more about RDDs in a future lab. For now, just remember this handy trick to figure out the number of partitions in a DataFrame."],"metadata":{}},{"cell_type":"markdown","source":["Here is what the DataFrame looks like:"],"metadata":{}},{"cell_type":"markdown","source":["![4 partitions](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/4_partitions_dashed.png)"],"metadata":{}},{"cell_type":"markdown","source":["The dashed lines for the borders indicates that the DataFrame is still on disk and has not been cached into memory."],"metadata":{}},{"cell_type":"markdown","source":["Count the number of records (rows) in the DataFrame:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.count()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Let's understand how Spark is actually computing the result of 7.2 million for the count action. It is important to understand the relationship between the number of partitions in a DataFrame and the number of tasks required to process a DataFrame."],"metadata":{}},{"cell_type":"markdown","source":["In the cell above, where you ran the count, expand the Spark Jobs and Stages:\n\n![Expand Jobs and Stages](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/count_jobs_stages_tasks.png)"],"metadata":{}},{"cell_type":"markdown","source":["Each Spark action (like count) kicks off one or more Jobs. Above we see that one job (Job #2) was launched. *(your specific job # may be different)*\n\nEach job is comprised of one or more Stages. Above we see that two stages (Stage #2 and #3) were launched to compute the result."],"metadata":{}},{"cell_type":"markdown","source":["To learn more details about the Job and Stages, open the Spark UI in a new tab:"],"metadata":{}},{"cell_type":"markdown","source":["![Open Spark UI](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/view-spark-ui.png)"],"metadata":{}},{"cell_type":"markdown","source":["When you go to the new \"Spark UI\" tab, you should see the Jobs page, with a few completed jobs. Click on the link under Description for the Job # used to run the count action:"],"metadata":{}},{"cell_type":"markdown","source":["![Two Completed Jobs](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/three_completed_jobs.png)"],"metadata":{}},{"cell_type":"markdown","source":["On the \"Details for Job #\" page, you can now see several metrics about the count Job:"],"metadata":{}},{"cell_type":"markdown","source":["![Two Stages Colored](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/two_stages_colored.png)"],"metadata":{}},{"cell_type":"markdown","source":["In the screenshot above, we can see (in purple) that Stage 2 (the first Stage for the count job) took 21 seconds to run, while Stage 3 only took 0.2 seconds.\n\nUnder the \"Input\" column, (in green) notice that Stage 2 read about 250 MB of data and (in orange) wrote 168 Bytes of shuffle data.\n\nUnder the \"Shuffle Read\" column, we can also see that Stage 3, (in orange) read the 168 Bytes of data that Stage 2 had written.\n\nTo learn more about the details of Stage 2, click the link (in red) under the Description column for Stage 2:"],"metadata":{}},{"cell_type":"markdown","source":["On the \"Details for Stage 2\" page, scroll all the way to the bottom till you see the 8 Tasks:"],"metadata":{}},{"cell_type":"markdown","source":["![Stage-1, 4 tasks](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/stageone_4tasks.png)"],"metadata":{}},{"cell_type":"markdown","source":["In the Tasks screenshot above, we can see (in green) that the first 3 tasks read 64 MB of the file, while the last task read 58 MB of the file. Also notice (in green) that the each of the 64 MB buffers that the first 3 tasks read was comprised of about 1.8 million records, but the last task that read 58 MB only read about 1.6 million records.\n\nWe can also see (in purple) that each task emitted a single 42 Byte record as the Shuffle Write."],"metadata":{}},{"cell_type":"markdown","source":["When Spark reads CSV files from S3, the input split is 64 MB. That means that Spark will launch a new task/thread to read each 64 MB split of the file.\n\nIn this case, after reading the first three input splits, only 58 MB remain, so the last task reads 58 MB:"],"metadata":{}},{"cell_type":"markdown","source":["![64 MB input split](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/input_split.png)"],"metadata":{}},{"cell_type":"markdown","source":["Click back in your browser to return to the \"Details of Job #\" page, then click on the link under Description to see the details of the next Stage (Stage #3):"],"metadata":{}},{"cell_type":"markdown","source":["![Click Stage 2](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/two_stages_clickstagetwo.png)"],"metadata":{}},{"cell_type":"markdown","source":["Once again, scroll all the way to the bottom till you see the 1 Task for Stage 3:"],"metadata":{}},{"cell_type":"markdown","source":["![Stage two, 1 task](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/stage2_onetask.png)"],"metadata":{}},{"cell_type":"markdown","source":["Notice in the screenshot above, that the single task in Stage 3 read 168 Bytes of data (4 records.)"],"metadata":{}},{"cell_type":"markdown","source":["The diagram below explains what's going on. The count Job is kicking off two stages.\n\nStage 2 (the first stage of the job) has 4 tasks and each task reads between 1.6 million to 1.8 million records.\n\nEach task in Stage 2 emits one record with the aggregated count that it saw in its local partition of data.\n\nThen all four tasks in Stage 2 complete.\n\nStage 3 (the second stage of the job) starts with only one task. The task reads the 4 records from Stage 2 and performs a final aggregation and emits the number 7.2 million back to our Notebook cell as the final result of the computation!"],"metadata":{}},{"cell_type":"markdown","source":["![Count, Physical Model](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/count_physicalmodelwjob.png)"],"metadata":{}},{"cell_type":"markdown","source":["####![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) **Transformation: `orderBy()`**"],"metadata":{}},{"cell_type":"markdown","source":["The `orderBy()` transformation can be used to order the table by the timestamp column:"],"metadata":{}},{"cell_type":"code","source":["(pageviews_df\n  .orderBy(\"timestamp\")  # transformation\n  .show(10)                            # action\n)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["The first 2 rows show data from March 16, 2015 at **00:00:00** (midnight).\n\nThe 3rd and 4th rows show data from a second after midnight, **00:00:01**.\n\nThe DataFrame contains 2 rows for every second, one for desktop and one for mobile."],"metadata":{}},{"cell_type":"markdown","source":["Did you notice that the first 6 rows in the DataFrame are ordered by `desktop`, then `mobile` traffic, but the last 4 rows are ordered by `mobile`, then `desktop`:"],"metadata":{}},{"cell_type":"markdown","source":["![Out of Order](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/out_of_order.png)"],"metadata":{}},{"cell_type":"markdown","source":["The following command orders the rows by first the timestamp (ascending), then the site (descending) and then shows the first 10 rows again:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.orderBy(pageviews_df[\"timestamp\"], pageviews_df[\"site\"].desc()).show(10)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["The `orderBy()` transformation takes from 20 to 30 seconds to run against the 255 MB pageviews file on S3."],"metadata":{}},{"cell_type":"markdown","source":["####![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) **Reading from Disk vs Memory**"],"metadata":{}},{"cell_type":"markdown","source":["The 255 MB pageviews file is currently on S3, which means each time you scan through it, your Spark cluster has to read the 255 MB of data remotely over the network."],"metadata":{}},{"cell_type":"markdown","source":["Once again, use the `count()` action to scan the entire 255 MB file from disk and count how many total records (rows) there are:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.count()"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["As we saw earlier, the pageviews DataFrame contains 7.2 million rows."],"metadata":{}},{"cell_type":"markdown","source":["Hmm, that took about at least 20 seconds. Let's cache the DataFrame into memory to speed it up."],"metadata":{}},{"cell_type":"code","source":["spark.table(\"pageviews_by_second\").cache()"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["Caching is a lazy operation (meaning it doesn't take effect until you call an action that needs to read all of the data). So let's call the `count()` action again:"],"metadata":{}},{"cell_type":"code","source":["# During this count() action, the data is read from S3 and counted, and also cached\n# Note that when the count action has to also cache data, it takes longer than simply a count (like above)\n\npageviews_df.count()"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":["The DataFrame should now be cached, let's run another `count()` to see the speed increase:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.count()"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["Notice that scanning the DataFrame takes significantly faster!"],"metadata":{}},{"cell_type":"markdown","source":["Now that the pageviews DataFrame is cached in memory, if you go to the Spark UI tab and click on \"Storage\" (1 in image below) you'll see the \"pageviews_by_second\" DataFrame in memory:"],"metadata":{}},{"cell_type":"markdown","source":["![Storage UI](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/storage_ui.png)"],"metadata":{}},{"cell_type":"markdown","source":["Notice above that the DataFrame is made of 4 partitions totaling 192 MB in size.\n\nThe Storage Level for DataFrames is actually the new Tungsten Binary format.\n\nClick on the DataFrame name link under the RDD Name column (2 in image above) to see more details about the DataFrame."],"metadata":{}},{"cell_type":"markdown","source":["![Storage UI](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/storage_ui_details.png)"],"metadata":{}},{"cell_type":"markdown","source":["Although the first 3 input splits read from S3 were 64 MB, when they got cached in memory using the Tungsten binary format, they became 49 MB each. The last 58 MB input split became a 44 MB partition in memory:"],"metadata":{}},{"cell_type":"markdown","source":["![df in memory](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/df_in_mem.png)"],"metadata":{}},{"cell_type":"markdown","source":["####![Wikipedia + Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/wiki_spark_small.png) Q-1) How many rows in the table refer to mobile vs desktop?"],"metadata":{}},{"cell_type":"markdown","source":["Use the `filter()` transformation to keep only the rows where the site column is equal to mobile:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.filter(pageviews_df['site'] == 'mobile').count()"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["**Challenge 1.**:\n\nOpen the Spark UI and answer the following questions:\n\n * How much time did the previous job take?\n * How many stages did is start?\n * How many bytes were read by the second stage?\n * How much time did the task in the second stage take to complete?"],"metadata":{}},{"cell_type":"code","source":["# TODO\n# Type your answers here...\n\n# Total time taken:\n# Number of stages started:\n# Shuffle bytes read in the second stage:\n# Time taken for the task in the second stage to complete:"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":["Expand the Spark Jobs above and notice that even though we added a `filter()` transformation, the Job still requires 2 Stages with 4 tasks in the first Stage and 1 task in the second Stage:"],"metadata":{}},{"cell_type":"markdown","source":["![Filter Count Expand](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/filter_count_expand.png)"],"metadata":{}},{"cell_type":"markdown","source":["**Challenge 2:** How many rows refer to desktop?"],"metadata":{}},{"cell_type":"code","source":["# TODO\n# Type your answer here..."],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":["So, 3.6 million rows refer to the mobile page views and 3.6 million rows refer to desktop page views."],"metadata":{}},{"cell_type":"markdown","source":["Let's compare the above `filter()` + `count()` from a Logical Model vs Physical Model perspective.\n\nReading a DataFrame from a Databricks table and running a filter() on it are both lazy operations, so technically no work is done yet:"],"metadata":{}},{"cell_type":"markdown","source":["![Logical Model: Filter](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/filter_lazy.png)"],"metadata":{}},{"cell_type":"markdown","source":["However, when you call the count() action, it triggers the read from S3, and the filter() and count() to run:"],"metadata":{}},{"cell_type":"markdown","source":["![Logical Model: Filter and Count](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/filter_count_run.png)"],"metadata":{}},{"cell_type":"markdown","source":["The Physical Model looks different. The filter() + count() job kicks off 2 Stages:"],"metadata":{}},{"cell_type":"markdown","source":["![Physical Model: Filter and Count](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/filter_physical_model.png)"],"metadata":{}},{"cell_type":"markdown","source":["Each of the four tasks in the 1st Stage are actually doing 4 things:\n- Read input split from S3\n- Filter for just mobile or desktop traffic\n- Do a local aggregation on the input split partition\n- Write a single record to local SSD with the count # seen in the partition"],"metadata":{}},{"cell_type":"markdown","source":["![Pipelining](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/pipelining.png)"],"metadata":{}},{"cell_type":"markdown","source":["####![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) ** SQL Query Plan Visualization & the Catalyst Optimizer**"],"metadata":{}},{"cell_type":"markdown","source":["Recall that the last command we just ran above was:"],"metadata":{}},{"cell_type":"code","source":["# pageviews_df.filter(pageviews_df['site'] == 'desktop').count()"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"markdown","source":["To see the SQL Query Plan for the `filter()` + `count()` query, click on the SQL tab in the Spark UI:"],"metadata":{}},{"cell_type":"markdown","source":["![SQL viz](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/sql_viz.png)"],"metadata":{}},{"cell_type":"markdown","source":["In the diagram above, you can see that 7.2 million records are read from memory (green) to create the DataFrame, then filtered for just desktop (or mobile) traffic. The 3.6 million rows that pass the filter are projected out to an aggregator, which outputs 4 records to the Shuffle.\n\nEverything above the TungstenExchange shuffle (in purple) is part of the 1st Stage. After the shuffle, in the 2nd stage, an aggregation is done on 4 input rows to emit 1 output row."],"metadata":{}},{"cell_type":"markdown","source":["You can expand the \"Details\" in the SQL visualization UI to see the logical and physical plans:"],"metadata":{}},{"cell_type":"markdown","source":["![SQL details](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/sql_details.png)"],"metadata":{}},{"cell_type":"markdown","source":["At the core of Spark SQL is the Catalyst optimizer, which all DataFrame, SQL and Dataset queries flow through to generate a physical plan that gets executed using RDDs:"],"metadata":{}},{"cell_type":"markdown","source":["![Catalyst Optimizer](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/catalyst.png)"],"metadata":{}},{"cell_type":"markdown","source":["Catalyst is one of the newest and most technically involved components of Spark. It leverages advanced programming language features (e.g. Scala?s pattern matching and quasiquotes) in a novel way to build an extensible query optimizer.\n\nThe main data type in Catalyst is a tree composed of zero or more child node objects. Trees can be manipulated using rules (functions that turn a tree into a new tree). You can read more about how Catalyst works in this Databricks blog post: <a href=\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" target=\"_blank\">April 2015: Deep Dive into Spark SQL?s Catalyst Optimizer</a>"],"metadata":{}},{"cell_type":"markdown","source":["**You can always check the logical and phisical plans of how Spark will calculate a DataFrame in the notebook, too**:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.filter(pageviews_df[\"site\"] == \"desktop\").explain(True)"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"markdown","source":["####![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) **Memory persistence and Shuffle Partitions **"],"metadata":{}},{"cell_type":"markdown","source":["Recall from the first notebook that your Spark local mode cluster is running with 3 slots:"],"metadata":{}},{"cell_type":"markdown","source":["![Notebook + Micro Cluster](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/book_intro/notebook_microcluster.png)"],"metadata":{}},{"cell_type":"markdown","source":["For best performance, we should cache DataFrames into memory with a number of partitions that is a multiple of the number of slots (3 or 6 or 9, etc).\n\nFor example, here is a DataFrame in memory (orange) with 3 partitions:"],"metadata":{}},{"cell_type":"markdown","source":["![Arch 3 slots](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/arch_3slots.png)"],"metadata":{}},{"cell_type":"markdown","source":["When running transformations on the DataFrame, all 3 partitions can be analyzed in parallel:"],"metadata":{}},{"cell_type":"markdown","source":["![Arch 3 tasks](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/arch_3tasks.png)"],"metadata":{}},{"cell_type":"markdown","source":["Three seems to be a more ideal number of partitions than four.\n\nFirst, unpersist the original base DataFrame, `pageviewsDF`. Then re-read the 255 MB file from S3, order it by the timestamp column, and re-cache it with 3 partitions:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.unpersist()"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"markdown","source":["The Storage UI will now be empty:"],"metadata":{}},{"cell_type":"markdown","source":["![Storage UI empty](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/storage_empty.png)"],"metadata":{}},{"cell_type":"markdown","source":["**Challenge 2:** Reload the table from S3, order it by the timestamp and site column (like above) and cache it:\n\nHint: Name the new DataFrame `pageviewsOrderedDF`"],"metadata":{}},{"cell_type":"code","source":["# TODO\n# Type your answer here..."],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"code","source":["# Materialize the cache\npageviews_ordered_df.count()"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"markdown","source":["How many partitions are in the new DataFrame?"],"metadata":{}},{"cell_type":"code","source":["pageviews_ordered_df.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"markdown","source":["#### **200 Partitions!**"],"metadata":{}},{"cell_type":"markdown","source":["What could have happened? Expand the Job details in the `count()` command above by clicking the two down arrows:"],"metadata":{}},{"cell_type":"markdown","source":["![200 tasks](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/200tasks.png)"],"metadata":{}},{"cell_type":"markdown","source":["The first stage (Stage 17 above) is reading the 4 input splits from S3. The next Stage seems to be using 200 tasks to do the `orderBy()` transformation (in purple). This is when the DataFrame is being snapshotted and cached into memory. The final stage (Stage 19 above) is doing the final aggregation for the count."],"metadata":{}},{"cell_type":"markdown","source":["By clicking on the Jobs tab in the Spark UI and then clicking into the details for the last job, you can see the same 3 stages:"],"metadata":{}},{"cell_type":"markdown","source":["![3stages_200partitions.png](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/3stages_200partitions.png)"],"metadata":{}},{"cell_type":"markdown","source":["Notice that the first stage read 250 MB of input data (in green) and wrote 110 MB of shuffle data (in purple).\n\nThe second stage read the 110 MB of shuffle data from the earlier stage and wrote 8.2 KB of shuffle data (in orange).\n\nThe third stage read the 8.2 KB of shuffle data from the middle stage."],"metadata":{}},{"cell_type":"markdown","source":["The trick to understanding what's going on lies in the following Spark SQL configuration option."],"metadata":{}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.shuffle.partitions\")"],"metadata":{},"outputs":[],"execution_count":136},{"cell_type":"markdown","source":["The option is set to 200. This configures the number of partitions to use when shuffling data for joins or aggregations.\n\nWe can change this value programmatically. What should be change it to? Some small multiple of the number of available threads in our cluster is a good starting point. Since we're using a local mode cluster, we can easily see how many threads we have."],"metadata":{}},{"cell_type":"code","source":["spark.conf.get(\"spark.master\")"],"metadata":{},"outputs":[],"execution_count":138},{"cell_type":"markdown","source":["Let's start with a multiple of 1, so we'll set the number of shuffle partitions to 8."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"],"metadata":{},"outputs":[],"execution_count":140},{"cell_type":"markdown","source":["Verify the change:"],"metadata":{}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.shuffle.partitions\")"],"metadata":{},"outputs":[],"execution_count":142},{"cell_type":"markdown","source":["Unpersist the DataFrame and re-run the read/orderBy/cache/count to store the DataFrame in memory with 3 partitions:"],"metadata":{}},{"cell_type":"code","source":["pageviews_ordered_df.unpersist()"],"metadata":{},"outputs":[],"execution_count":144},{"cell_type":"code","source":["pageviews_ordered_df = (\n  spark\n    .read\n    .table(\"pageviews_by_second\")\n    .orderBy(pageviews_ordered_df['timestamp'], pageviews_ordered_df['site'].desc())\n    .cache()\n)"],"metadata":{},"outputs":[],"execution_count":145},{"cell_type":"code","source":["pageviews_ordered_df.count()"],"metadata":{},"outputs":[],"execution_count":146},{"cell_type":"markdown","source":["Expand the Spark Jobs and Job # details in the cell above and note that this time the middle stage only used 3 tasks:"],"metadata":{}},{"cell_type":"markdown","source":["![Middle Stage, 3 Tasks](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/middlestage_3tasks.png)"],"metadata":{}},{"cell_type":"markdown","source":["Check the size of the DataFrame now:"],"metadata":{}},{"cell_type":"code","source":["pageviews_ordered_df.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":150},{"cell_type":"markdown","source":["If you drill into the details of the Spark UI's Storage tab and click on the RDD name, you will now see the DataFrame in memory with 3 partitions:"],"metadata":{}},{"cell_type":"markdown","source":["![Storage UI, 3 tasks](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/storage_3partitions.png)"],"metadata":{}},{"cell_type":"markdown","source":["You can learn more about the different configuration options in Spark SQL in the <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html#other-configuration-options\" target=\"_blank\">Apache Spark Docs</a>."],"metadata":{}}],"metadata":{"name":"02-DE-Pageviews","notebookId":945777816841470},"nbformat":4,"nbformat_minor":0}
