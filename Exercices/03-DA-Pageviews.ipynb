{"cells":[{"cell_type":"markdown","source":["#![Spark Logo](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark.png)\n\n**Objective:**\nAnalyze Desktop vs Mobile traffic to English Wikipedia (continued)\n\n**Time to Complete:**\n20 mins\n\n**Data Source:**\npageviews_by_second (<a href=\"http://datahub.io/en/dataset/english-wikipedia-pageviews-by-second\" target=\"_blank\">255 MB</a>)\n\n**Business Questions:**\n\n* Question # 1) How many total incoming requests were to the mobile site vs the desktop site?\n* Question # 2) What is the start and end range of time for the pageviews data? How many days of data is in the DataFrame?\n* Question # 3) What is the avg/min/max for the number of requests received for Mobile and Desktop views?\n* Question # 4) Which day of the week does Wikipedia get the most traffic?\n* Question # 5) Can you visualize both the mobile and desktop site requests in a line chart to compare traffic between both sites by day of the week?\n* Question # 6) Why is there so much more traffic on Monday vs. other days of the week?\n\n**Technical Accomplishments:**\n- Give a DataFrame a human-readable name when caching\n- Cast a String col type into a Timestamp col type\n- Browse the Spark SQL API docs\n- Learn how to use \"Date time functions\"\n- Create and use a User Defined Function (UDF)\n- Make a Databricks bar chart visualization\n- Join 2 DataFrames\n- Make a Matplotlib visualization\n\n**NOTE** Please run this notebook in a Spark 2.0 cluster."],"metadata":{}},{"cell_type":"markdown","source":["Attach to, and then restart your cluster first to clear out old memory caches and get to a default, standard environment. The restart should take 1 - 2 minutes.\n\n![Restart Cluster](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/restart_cluster.png)"],"metadata":{}},{"cell_type":"markdown","source":["####![Wikipedia Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_wikipedia_tiny.png) ![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) **Continue Exploring Pageviews By Second**"],"metadata":{}},{"cell_type":"markdown","source":["In this notebook, we will continue exploring the Wikipedia pageviews by second data."],"metadata":{}},{"cell_type":"markdown","source":["First, change the shuffle.partitions option to 8:"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["We're going to read the raw pageviews-by-second data. This data is stored in a TSV (tab-separated value), so we can read it using\nthe add-on [Spark CSV](https://spark-packages.org/package/databricks/spark-csv) package. (Note: In Spark 2.0, this package is no longer an add-on; it's built-in.)"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/databricks-datasets/wikipedia-datasets/data-001/pageviews/raw\"))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["However, we don't want to let Spark CSV infer the schema, because that requires two passes over the data file. The file isn't huge, but it's more than 200 MB, so two passes is a little slow, especially for a class. So, we're going to specify the schema ourselves. Let's take a quick look at the first couple lines of the file:"],"metadata":{}},{"cell_type":"code","source":["display(\n  spark.read.text(\"dbfs:/databricks-datasets/wikipedia-datasets/data-001/pageviews/raw/pageviews_by_second.tsv\")\n)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Okay, so the fields appear to be:\n\n* The timestamp, in ISO 8601 format. Unfortunately, Spark SQL can't parse this automatically, so we'll have to treat it as a string and do something ugly (and tricky) to make it into a timestamp.\n* The site, which is just \"mobile\" or \"desktop\". So that's obviously a string.\n* The request count, which is an integer.\n\nWe can use this information to craft our own schema, as shown below."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema = StructType([\n  StructField(\"timestamp\", StringType(), True),\n  StructField(\"site\", StringType(), True),\n  StructField(\"requests\", IntegerType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["As noted above, Spark SQL cannot parse ISO 8601 timestamps. _We_ can, using a `java.text.SimpleDateFormat` format string, but there's no `date_parse` SQL function in the Spark SQL API.\n\nHowever...\n\nYou _can_ parse it with relatively simple workaround. First, you need to use the Spark SQL `unix_timestamp` function, along with\na [SimpleDateFormat](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html), to parse the ISO 8601 timestamp\ninto a number representing the _seconds_ since January 1, 1970.\n\nThen, you can simply cast the resulting integer to a timestamp."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ndf0 = (spark.read\n            .option('delimiter', \"\\t\")\n            .option('header', 'true')\n            .schema(schema)\n            .csv('dbfs:/databricks-datasets/wikipedia-datasets/data-001/pageviews/raw/pageviews_by_second.tsv'))\ndf = (df0.select(df0['site'], df0['requests'],\n                 unix_timestamp(df0['timestamp'], \"yyyy-MM-dd'T'HH:mm:ss\").cast('timestamp').alias('timestamp')))\n# Note: In versions of Spark prior to 2.0, you'll need to use this code, instead:\n\n#df0 = (sqlContext.read\n#                 .format('com.databricks.spark.csv')\n#                 .option('delimiter', \"\\t\")\n#                 .option('header', 'true')\n#                 .schema(schema)\n#                 .load('dbfs:/databricks-datasets/wikipedia-datasets/data-001/pageviews/raw/pageviews_by_second.tsv'))\n#df = (df0.select(df0['site'], df0['requests'],\n#                 unix_timestamp(df0['timestamp'], \"yyyy-MM-dd'T'HH:mm:ss\").cast('timestamp').alias('timestamp')))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["df.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Now, let's put the data in a more useful order."],"metadata":{}},{"cell_type":"code","source":["pageviews_df = df.orderBy(df['timestamp'], df['site'].desc())"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Create a temporary view.\n\n**NOTE**: In Spark 2.0, `registerTempTable` has been renamed to the `createOrReplaceTempView`, which more accurately reflects what it actually does."],"metadata":{}},{"cell_type":"code","source":["pageviews_df.createOrReplaceTempView(\"pageviews_by_second_ordered\")\nspark.table(\"pageviews_by_second_ordered\").cache()\n\n# In versions of Spark prior to 2.0, use the following code:\n\n#pageviews_df.registerTempTable('pageviews_by_second_ordered')\n#sqlContext.cacheTable('pageviews_by_second_ordered')"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Materialize the cache with a `count()` action:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.count() # materialize the cache"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["You should now see the DataFrame in the Storage UI:"],"metadata":{}},{"cell_type":"markdown","source":["![Clean Name and 3 partitions](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/pageviews_cleanname_3partitions.png)"],"metadata":{}},{"cell_type":"markdown","source":["Look at the first 6 rows:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.show(6)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Verify that the DataFrame is indeed in memory by running a count again:"],"metadata":{}},{"cell_type":"code","source":["# This should run in less than a second.\npageviews_df.count()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["####![Wikipedia + Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/wiki_spark_small.png) Q-1) How many total incoming requests were to the mobile site vs the desktop site?"],"metadata":{}},{"cell_type":"markdown","source":["First, let's sum up the `requests` column to see how many total requests are in the dataset. We've already imported the SQL functions package, which includes statistical functions like `sum`, `max`, `min`, `avg`, etc."],"metadata":{}},{"cell_type":"code","source":["pageviews_df.select(sum(pageviews_df['requests'])).show()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["So, there are about 13.3 billion requests total."],"metadata":{}},{"cell_type":"markdown","source":["But how many of the requests were for the mobile site?"],"metadata":{}},{"cell_type":"markdown","source":["**Challenge 1:** Using just the commands we learned so far, can you figure out how to filter the DataFrame for just **mobile** traffic and then sum the requests column?"],"metadata":{}},{"cell_type":"code","source":["# TODO\n# Type your answer here"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["**Challenge 2:** What about the **desktop** site? How many requests did it get?"],"metadata":{}},{"cell_type":"code","source":["# TODO\n# Type your answer here"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["So, about twice as many were for the desktop site."],"metadata":{}},{"cell_type":"markdown","source":["####![Wikipedia + Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/wiki_spark_small.png) Q-2) What is the start and end range of time for the pageviews data? How many days of data is in the DataFrame?"],"metadata":{}},{"cell_type":"markdown","source":["How many different years is the data from?"],"metadata":{}},{"cell_type":"markdown","source":["For the next command, we'll use `year()`, one of the date time function available in Spark. You can review which functions are available for DataFrames in the <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">Spark API doc's SQL functions</a>, under \"Date time functions\"."],"metadata":{}},{"cell_type":"code","source":["pageviews_df.select(year(pageviews_df[\"timestamp\"])).distinct().show()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["The data only spans 2015. But which months?"],"metadata":{}},{"cell_type":"markdown","source":["**Challenge 3:** Can you figure out how to check which months of 2015 the data covers (using the <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">Spark API docs</a>)?"],"metadata":{}},{"cell_type":"code","source":["# TODO\n# Type your answer here"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["The data covers the months you see above."],"metadata":{}},{"cell_type":"markdown","source":["Similarly, you can discover how many weeks of timestamps are in the data and how many days of data there is:"],"metadata":{}},{"cell_type":"code","source":["# How many weeks of data are there?\npageviews_df.select(weekofyear(pageviews_df[\"timestamp\"])).distinct().show()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["# How many days of data are there?\npageviews_df.select(dayofyear(pageviews_df[\"timestamp\"])).distinct().count()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["There is 41 days of data."],"metadata":{}},{"cell_type":"markdown","source":["####![Wikipedia + Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/wiki_spark_small.png) Q-3) What is the avg/min/max for the number of requests received for Mobile and Desktop views?"],"metadata":{}},{"cell_type":"markdown","source":["**Challenge 4**: To understand our data better, let's look at the average, minimum and maximum number of requests received for mobile, then desktop:"],"metadata":{}},{"cell_type":"code","source":["# TODO\n\n# Look at mobile statistics: select the average, minimum and maximum number of requests\n# Your answer here..."],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# TODO\n\n# Look at desktop statistics: select the average, minimum and maximum number of requests\n# Your answer here..."],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["There certainly appear to be more requests for the desktop site."],"metadata":{}},{"cell_type":"markdown","source":["By the way, there's a really simple way to get these kinds of summary statistics on numeric columns:"],"metadata":{}},{"cell_type":"code","source":["pageviews_df.filter(\"site = 'mobile'\").describe().show()"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["####![Wikipedia + Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/wiki_spark_small.png) Q-4) Which day of the week does Wikipedia get the most traffic?"],"metadata":{}},{"cell_type":"markdown","source":["Think about how we can accomplish this. We need to pull out the day of the week (like Mon, Tues, etc) from each row, and then sum up all of the requests by day."],"metadata":{}},{"cell_type":"markdown","source":["First, use the `date_format` function to extract out the day of the week from the timestamp and rename the column as \"Day of week\".\n\nThen we'll sum up all of the requests for each day and show the results."],"metadata":{}},{"cell_type":"code","source":["# Notice the use of alias() to rename the new column\n# \"E\" is a pattern in the SimpleDataFormat class in Java that extracts out the \"Day in Week\"\"\n\n# Create a new DataFrame named pageviews_by_day_of_week_df and cache it\npageviews_by_day_of_week_df = pageviews_df.groupBy(date_format((pageviews_df[\"timestamp\"]), \"E\").alias(\"Day of week\")).sum()\n\n# Cache the DataFrame with a human-readable name\npageviews_by_day_of_week_df.createOrReplaceTempView(\"pageviews_by_DOW\")\nspark.table(\"pageviews_by_DOW\").cache()\n\n# Prior to Spark 2.0:\n#pageviews_by_day_of_week_df.registerTempTable(\"pageviews_by_DOW\")\n#sqlContext.cacheTable(\"pageviews_by_DOW\")\n\n# Show what is in the new DataFrame\npageviews_by_day_of_week_df.show()"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["You can learn more about date/time patterns, like \"E\", in the <a href=\"https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html\" target=\"_blank\">Java SimpleDateFormat</a> documentation."],"metadata":{}},{"cell_type":"markdown","source":["It would help to visualize the results:"],"metadata":{}},{"cell_type":"code","source":["display(pageviews_by_day_of_week_df.orderBy(\"Day of week\"))"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["Click on the Bar chart icon above to convert the table into a bar chart:\n\n#![Bar Chart](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/barchart_icon.png)"],"metadata":{}},{"cell_type":"markdown","source":["Under the \"Plot Options\" button above, you might also need to set the Keys as \"Day of week\" and the values as \"sum(requests)\"."],"metadata":{}},{"cell_type":"markdown","source":["Hmm, the ordering of the days of the week is off, because the `orderBy()` operation is ordering the days of the week alphabetically. Instead of that, let's start with Monday and end with Sunday. To accomplish this, we'll write a short User Defined Function (UDF) to prepend each `Day of week` with a number."],"metadata":{}},{"cell_type":"markdown","source":["####![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) **User Defined Functions (UDFs)**"],"metadata":{}},{"cell_type":"markdown","source":["A UDF lets you code your own logic for processing column values during a DataFrame query.\n\nFirst, let's create a function to map a named day of the week into something with a numeric prefix:"],"metadata":{}},{"cell_type":"code","source":["_dow = {\"Mon\": \"1\", \"Tue\": \"2\", \"Wed\": \"3\", \"Thu\": \"4\", \"Fri\": \"5\", \"Sat\": \"6\", \"Sun\": \"7\"}\ndef map_day_of_week(day):\n  n = _dow.get(day)\n  if n:\n    return n + \"-\" + day\n  else:\n    return \"UNKNOWN\""],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["Test the match expression:"],"metadata":{}},{"cell_type":"code","source":["map_day_of_week(\"Tue\")"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["Great, it works! Now define a UDF that wraps this function:"],"metadata":{}},{"cell_type":"code","source":["prepend_number_udf = udf(map_day_of_week)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["Test the UDF to prepend the `Day of Week` column in the DataFrame with a number:"],"metadata":{}},{"cell_type":"code","source":["pageviews_by_day_of_week_df.select(prepend_number_udf(pageviews_by_day_of_week_df[\"Day of week\"])).show(7)"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["Our UDF looks like it's working. Next, let's apply the UDF and also order the x axis from Mon -> Sun:"],"metadata":{}},{"cell_type":"code","source":["df0 = pageviews_by_day_of_week_df.withColumnRenamed(\"sum(requests)\", \"total requests\")\ndisplay(\n  df0.select(prepend_number_udf(df0[\"Day of week\"]).alias(\"dow\"), df0[\"total requests\"])\n     .orderBy(\"dow\")\n)"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["Click on the bar chart icon again to convert the above table into a Bar Chart. Also, under the Plot Options, you may need to set the Keys as \"UDF(Day of week)\" and the values as \"total requests\"."],"metadata":{}},{"cell_type":"markdown","source":["Wikipedia seems to get significantly more traffic on Mondays than other days of the week. Hmm..."],"metadata":{}},{"cell_type":"markdown","source":["### This is a _completely unnecessary_ UDF.\n\nIn general, don't write UDFs if built-in functions will do. We could have accomplished the exact same thing using one of the built-in\nfunctions in `org.apache.spark.sql.functions`.\n\n**Challenge 4**: Take a look at <http://spark.apache.org/docs/1.6.2/api/scala/index.html#org.apache.spark.sql.functions$> and try to figure out a way to create the same plot _without_ using a UDF."],"metadata":{}},{"cell_type":"code","source":["# TODO\n# Type your answer here"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["####![Wikipedia + Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/wiki_spark_small.png) Q-5) Can you visualize both the mobile and desktop site requests in a line chart to compare traffic between both sites by day of the week?"],"metadata":{}},{"cell_type":"markdown","source":["First, graph the mobile site requests:"],"metadata":{}},{"cell_type":"code","source":["df0 = (pageviews_df.filter(\"site = 'mobile'\")\n                   .groupBy(date_format((pageviews_df[\"timestamp\"]), \"u-E\").alias(\"Day of week\"))\n                   .sum()\n                   .withColumnRenamed(\"sum(requests)\", \"mobile requests\"))\nmobile_views_by_day_of_week_df = df0.select(df0[\"Day of week\"], df0[\"mobile requests\"]).orderBy(\"Day of week\")"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["display(mobile_views_by_day_of_week_df)"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["Click on the bar chart icon again to convert the above table into a Bar Chart.\n\nAlso, under the Plot Options, you may need to set the Keys as \"DOW\" and the values as \"mobile requests\"."],"metadata":{}},{"cell_type":"markdown","source":["With a DataFrame for mobile views, let's create one more for desktops:"],"metadata":{}},{"cell_type":"code","source":["df0 = (pageviews_df.filter(\"site = 'desktop'\")\n                   .groupBy(date_format((pageviews_df[\"timestamp\"]), \"u-E\").alias(\"Day of week\"))\n                   .sum()\n                   .withColumnRenamed(\"sum(requests)\", \"desktop requests\"))\ndesktop_views_by_day_of_week_df = df0.select(df0[\"Day of week\"], df0[\"desktop requests\"]).orderBy(\"Day of week\")"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":["We now have two DataFrames:\n* **desktopViewsByDayOfWeekDF**\n* **mobileViewsByDayOfWeekDF**\n\nWe can then perform a join on the two DataFrames to create a thrid DataFrame, **allViewsByDayOfWeekDF**"],"metadata":{}},{"cell_type":"code","source":["all_views_by_day_of_week_df = (\n  mobile_views_by_day_of_week_df\n    .join(desktop_views_by_day_of_week_df,\n          mobile_views_by_day_of_week_df[\"Day of week\"] == desktop_views_by_day_of_week_df[\"Day of week\"])\n)"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":["And lastly, we can create a line chart to visualize mobile vs. desktop page views:"],"metadata":{}},{"cell_type":"code","source":["display(all_views_by_day_of_week_df)"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":["Click on the line chart icon above to convert the table into a line chart:\n\n#![Line Chart 1](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/linechart_1.png)"],"metadata":{}},{"cell_type":"markdown","source":["Then click on Plot Options:\n\n#![Line Chart 2](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/linechart_2.png)"],"metadata":{}},{"cell_type":"markdown","source":["Finally customize the plot as seen below and click Apply:\n\n*(You will have to drag and drop fields from the left pane into either Keys or Values)*\n\n#![Line Chart 3](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/linechart_3.png)"],"metadata":{}},{"cell_type":"markdown","source":["Hmm, did you notice that the line chart is a bit deceptive? Beware that it looks like there were almost zero mobile site requests because the y-axis of the line graph starts from 600,000,000 instead of 0.\n\n<img src=\"http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/pageviews/linechart_4.png\" style=\"height: 300px\" alt=\"Line chart 4\"/>"],"metadata":{}},{"cell_type":"markdown","source":["Since the y-axis is off, it may appear as if there were almost zero mobile site requests. We can restore a zero baseline by using Matplotlib. But first..."],"metadata":{}},{"cell_type":"markdown","source":["####![Wikipedia + Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/wiki_spark_small.png) Q-6) Why is there so much more traffic on Monday vs. other days of the week?"],"metadata":{}},{"cell_type":"markdown","source":["** Challenge 5:** Can you figure out exactly why there was so much more traffic on Mondays?"],"metadata":{}},{"cell_type":"code","source":["# Type your answer here"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"markdown","source":["####![Databricks Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_databricks_tiny.png) Bonus: Matplotlib visualization"],"metadata":{}},{"cell_type":"markdown","source":["Let's use Matplotlib to fix the line chart visualization above so that the y-axis starts with 0."],"metadata":{}},{"cell_type":"markdown","source":["You can also import Matplotlib and easily create more sophisticated plots:"],"metadata":{}},{"cell_type":"code","source":["%python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig1, ax = plt.subplots()\n\n# The first list of four numbers is for the x-axis and the next list is for the y-axis\nax.plot([1,2,3,4], [1,4,9,16])\n\ndisplay(fig1)"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"markdown","source":["Recall that we had earlier created 2 DataFrames, one with desktop views by day of week and another with mobile views by day of week:"],"metadata":{}},{"cell_type":"code","source":["desktop_views_by_day_of_week_df.show()"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["mobile_views_by_day_of_week_df.show()"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"markdown","source":["First let's graph only the desktop views by day of week:"],"metadata":{}},{"cell_type":"code","source":["%python\nfig2, ax = plt.subplots()\n\n# Notice that we are providing the coordinate manually for the x-axis\nax.plot([0,1,2,3,4,5,6], [1566792176,1346947425,1346330702,1306170813,1207342832,1016427413,947169611], 'ro')\n\n# The axis() command takes a list of [xmin, xmax, ymin, ymax] and specifies the viewport of the axes\nax.axis([0, 7, 0, 2000000000])\n\ndisplay(fig2)"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"markdown","source":["Next graph only the mobile views by day of week:"],"metadata":{}},{"cell_type":"code","source":["%python\nfig3, ax = plt.subplots()\nax.plot([0,1,2,3,4,5,6], [790026669,648087459,631284694,625338164,635169886,646334635,629556455], 'bo')\n\n# The axis() command takes a list of [xmin, xmax, ymin, ymax] and specifies the viewport of the axes\nax.axis([0, 7, 0, 2000000000])\n\ndisplay(fig3)"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"markdown","source":["Finally, let's combine the 2 plots above and also programatically get the requests data from a DataFrame (instead of manually entering the y-axis corrdinates).\n\nWe need a technique to access the Scala DataFrames from the Python cells. To do this, we can register a temporary table in Scala, then call that table from Python."],"metadata":{}},{"cell_type":"code","source":["mobile_views_by_day_of_week_df.createOrReplaceTempView(\"mobileViewsByDOW\")\ndesktop_views_by_day_of_week_df.createOrReplaceTempView(\"desktopViewsByDOW\")\n\n# Remember: Prior to Spark 2.0, you have to use:\n#mobile_views_by_day_of_week_df.registerTempTable(\"mobileViewsByDOW\")\n#desktop_views_by_day_of_week_df.registerTempTable(\"desktopViewsByDOW\")"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["%python\nmobile_df = spark.read.table(\"mobileViewsByDOW\")\nmobile_requests = [list(r)[1] for r in mobile_df.collect()]\n\ndesktop_df = spark.read.table(\"desktopViewsByDOW\")\ndesktop_requests = [list(r)[1] for r in desktop_df.collect()]\n\nprint(mobile_requests, desktop_requests)"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"markdown","source":["We now have our two Python lists. We can use them to plot."],"metadata":{}},{"cell_type":"code","source":["%python\nfig3, ax = plt.subplots()\n\nx_axis = [0,1,2,3,4,5,6]\n\nax.plot(x_axis, desktop_requests, marker='o', linestyle='--', color='r', label='Desktop')\nax.plot(x_axis, mobile_requests, marker='o', label='Mobile')\n\nax.set_title('Desktop vs Mobile site requests')\n\nax.set_xlabel('Days of week')\nax.set_ylabel('# of requests')\n\nax.legend()\n\n# The axis() command takes a list of [xmin, xmax, ymin, ymax] and specifies the viewport of the axes\nax.axis([0, 6, 0, 2000000000])\n\nax.xaxis.set_ticks(range(len(x_axis)), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n\ndisplay(fig3)"],"metadata":{},"outputs":[],"execution_count":117}],"metadata":{"name":"03-DA-Pageviews","notebookId":945777816841625},"nbformat":4,"nbformat_minor":0}
