{"cells":[{"cell_type":"markdown","source":["# Programming Spark with Python\n\nBelow you'll find links to PySpark API documentation, and see some of the common Python idioms, expressions and language features used when programming Spark. We assume you are comfortable with the basics of Python already, covering the intermediate language features used in Spark."],"metadata":{}},{"cell_type":"markdown","source":["## Spark Programming Guides\n* <a href=\"http://spark.apache.org/docs/1.6.1/programming-guide.html#resilient-distributed-datasets-rdds\" target=\"_blank\">RDDs</a>\n* <a href=\"http://spark.apache.org/docs/1.6.1/sql-programming-guide.html\" target=\"_blank\">DataFrames</a>\n* <a href=\"http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html\" target=\"_blank\">Streaming</a>\n* <a href=\"http://spark.apache.org/docs/1.6.1/ml-guide.html\" target=\"_blank\">Machine Learning Pipelines</a>\n\n## PySpark API Docs\n\n* <a href=\"http://spark.apache.org/docs/1.6.1/api/python/\" target=\"_blank\">PySpark API Docs Home</a>\n* <a href=\"http://spark.apache.org/docs/1.6.1/api/python/#core-classes\" target=\"_blank\">Core classes in PySpark</a>\n* <a href=\"https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD\" target=\"_blank\">RDDs</a>\n* <a href=\"http://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.DataFrame\" target=\"_blank\">DataFrames</a>\n* <a href=\"http://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#module-pyspark.sql.functions\" target=\"_blank\">SQL Functions</a> (used on DataFrame columns)\n* <a href=\"http://spark.apache.org/docs/1.6.1/api/python/pyspark.streaming.html\" target=\"_blank\">Streaming</a>\n* <a href=\"http://spark.apache.org/docs/1.6.1/api/python/pyspark.ml.html\" target=\"_blank\">Machine Learning Pipelines</a>\n\n\n## Python Language Documentation\n* <a href=\"https://docs.python.org/2/library/functions.html\" target=\"_blank\">Python built-in functions</a>\n* <a href=\"https://docs.python.org/2/library/\" target=\"_blank\">Python standard library</a> (included with Python itself)\n* <a href=\"https://docs.python.org/2/library/stdtypes.html#string-methods\" target=\"_blank\">Python string methods</a> (often useful in ETL)"],"metadata":{}},{"cell_type":"markdown","source":["# Passing Functions\n\nIn Python, everything is an object... including functions. You can pass a function as an argument to another function. This is commonly used in big data programming, and Spark is no different."],"metadata":{}},{"cell_type":"code","source":["# Here is some data...\nnumbers = [2, 8, -1, 3, 4, -12, 7]\n\n# And here are some functions.\ndef double(number):\n  return number * 2\ndef is_positive(number):\n  return number > 0\ndef add(a, b):\n  return a + b\n\n# We can use this with mapping operations.\n# Notice we write \"double\", not \"double()\" - no parentheses!\nprint(\"Doubling the numbers:\")\nprint(map(double, numbers))\n\n# You can, of course, use built-in functions too. For example, abs(), for absolute value.\nprint(\"Absolute values:\")\nprint(map(abs, numbers))\n\n# Filter operations require a function that, when called, return True or False.\nprint(\"Positive numbers:\")\nprint(filter(is_positive, numbers))\n\n# Reduce operations take a combination function and a sequence, and return a derived value.\n# Unlike map and filter, reduce requires a function taking *two* arguments, not one.\nprint(\"Sum of numbers:\")\nprint(reduce(add, numbers))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["# Lambdas\nSometimes we want to use a function object just once, and would rather not define it somewhere else. For convenience and (sometimes) improved readability, Python lets you create anonymous functions, or *lambdas*.\n\nThe syntax looks like:\n\n    lambda n: n + 2\n\nNotice that:\n\n* You start with the keyword `lambda`.\n* There is no return statement.\n* This entire expression evaluates to a function object. It can be passed to map, filter, etc.\n\nLet's see how this is used in code."],"metadata":{}},{"cell_type":"code","source":["# Again, the same numbers...\nnumbers = [2, 8, -1, 3, 4, -12, 7]\n\n# You use the lambda expression in the same place you would \n# normally use a function.\nprint(\"Doubling the numbers:\")\nprint(map(lambda number: number * 2, numbers))\n\n# Remember, Python lambdas have no return statement. The part to the right\n# of the : is automatically returned.\nprint(\"Positive numbers:\")\nprint(filter(lambda number: number > 0, numbers))\n\n# Lambdas can take several arugments.\nprint(\"Sum of numbers:\")\nprint(reduce(lambda a, b: a+b, numbers))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["In Python, lambdas are syntactically limited to one line (in contrast to Scala, for example, which lets lambdas be of any length). The opinion of Python's creators is that large anonymous functions quickly become unreadable; regardless of whether you agree, it's worth considering readability when choosing whether to use a lambda, or define a separate function. For short expressions, lambdas are often at least as readable; for longer, more complex logic, a separate function may be a better choice."],"metadata":{}},{"cell_type":"markdown","source":["# Named Tuples\n\nAs you likely know, Python includes a tuple type, which is like a list, but immutable.\n\n    # (name, gpa, major)\n    student_info = (\"John Doe\", 3.8, \"chemistry\")\n    # You can write student_info[0], but not student_info.append(...)\n\nThis lets us conveniently work with records as immutable, ordered fields of data. In PySpark programming, we often find it valuable to use an extension called a [namedtuple](https://docs.python.org/2/library/collections.html#collections.namedtuple). It is in the Python `collections` module:\n\n    from collections import namedtuple\n    \nThis works much like a tuple, but also lets us reference the fields by readable names, instead of obscure numeric indices."],"metadata":{}},{"cell_type":"code","source":["# First, import it.\nfrom collections import namedtuple\n\n# To use, we first create a namedtuple instance, giving it a specific name.\n# The first argument is a string, and normally the name of the type we assign it to.\n# The second argument is a list of strings, which (in order) are the field names.\nStudent = namedtuple('Student', ['name', 'gpa', 'major'])\n\n# This lets us create Student objects, with a syntax similar to if we had defined a Student class.\nstudent_john = Student(\"John Doe\", 3.8, \"chemistry\")\n\n# We can reference its fields by name:\nprint(\"Using namedtuple fields...\")\nprint(\"Student name: \" + student_john.name)\nprint(\"Student GPA: \" + str(student_john.gpa))\nprint(\"Student major: \" + student_john.major)\n\n# Since it's a tuple, we can also reference the fields by index if we need to:\nprint(\"\\nUsing numeric indices...\")\nprint(\"Student name: \" + student_john[0])\nprint(\"Student GPA: \" + str(student_john[1]))\nprint(\"Student major: \" + student_john[2])"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["# String Operations\n\nEspecially when loading and transforming data, you will often need to munge some text. Python's string type (called `str`) has \n<a href=\"https://docs.python.org/2/library/stdtypes.html#string-methods\" target=\"_blank\">many built-in methods</a>. Note none of them modify the original string; they instead create and return a new, different string. Here are some that you may find particularly useful:"],"metadata":{}},{"cell_type":"code","source":["poem = \"  Beauty is truth; truth, beauty. \\t \"\nunderscored = \"__Beauty is truth; truth, beauty.__\"\nadvice = \"Genius without education is like silver in the mine.\"\n\n# strip: Without args, strip whitespace from the front and end of a line.\nprint(\"Stripped:\")\nprint('<' + poem.strip() + '>')\n\n# You can also use .lstrip() and .rstrip() to just strip one side.\nprint(\"Left-stripped:\")\nprint('<' + poem.lstrip() + '>')\nprint(\"Right-stripped:\")\nprint('<' + poem.rstrip() + '>')\n\n# Pass in an argument to strip a specific character.\nprint(\"Stripped of underscores:\")\nprint(underscored.strip(\"_\"))\n\n# To split a string into a list of words, use .split().\nprint(\"Split:\")\nprint(advice.split())\n# By default it splits on whitespace.\n# Pass in an argument to split by a different character:\nprint(\"Split by semicolon:\")\nprint(poem.split(\";\"))\n\n# To go the opposite direction, use .join().\npets = [\"dog\", \"cat\", \"bird\", \"goat\", \"llama\"]\nprint(\"Pets:\")\nprint(\", \".join(pets))\n\n# .upper() and .lower() are often useful for normalizing data.\nprint(\"whispered advice:\")\nprint(advice.lower())\nprint(\"SHOUTED ADVICE:\")\nprint(advice.upper())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"Python-Guide","notebookId":945777816841959},"nbformat":4,"nbformat_minor":0}
