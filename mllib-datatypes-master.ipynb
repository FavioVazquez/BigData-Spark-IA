{"cells":[{"cell_type":"markdown","source":["# MLlib Data Types\n\nThis notebook explains the machine learning specific data types in Spark.  The focus is on the data types and classes used for generating models.  These include: `DenseVector`, `SparseVector`, `LabeledPoint`, and `Rating`.\n\nFor reference:\n\nThe [MLlib Guide](http://spark.apache.org/docs/latest/mllib-guide.html) provides an overview of all aspects of MLlib and [MLlib Guide: Data Types](http://spark.apache.org/docs/latest/mllib-data-types.html) provides a detailed review of data types specific for MLlib\n\nAfter this lab you should understand the differences between `DenseVectors` and `SparseVectors` and be able to create and use `DenseVector`, `SparseVector`, `LabeledPoint`, and `Rating` objects.  You'll also learn where to obtain additional information regarding the APIs and specific class / method functionality."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib import linalg\ndir(linalg)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["help(linalg)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["#### Dense and Sparse\n\nMLlib supports both dense and sparse types for vectors and matrices.  We'll focus on vectors as they are most commonly used in MLlib and matrices have poor scaling properties.\n\nA dense vector contains an array of values, while a sparse vector stores the size of the vector, an array of indices, and an array of values that correspond to the indices.  A sparse vector saves space by not storing zero values.\n\nFor example, if we had the dense vector `[2.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0]`, we could store that as a sparse vector with size 7, indices as `[0, 3]`, and values as `[2.0, 3.0]`."],"metadata":{}},{"cell_type":"code","source":["# import data types\nfrom pyspark.mllib.linalg import DenseVector, SparseVector, SparseMatrix, DenseMatrix, Vectors, Matrices"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["A great way to get help when using Python is to use the help function on an object or method.  If help isn't sufficient, other places to look include the [programming guides](http://spark.apache.org/docs/latest/programming-guide.html), the [Python API](http://spark.apache.org/docs/latest/api/python/index.html), and directly in the [source code](https://github.com/apache/spark/tree/master/python/pyspark) for PySpark."],"metadata":{}},{"cell_type":"code","source":["help(Vectors.dense)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### DenseVector"],"metadata":{}},{"cell_type":"markdown","source":["PySpark provides a [DenseVector](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseVector) class within the module [pyspark.mllib.linalg](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.linalg).  `DenseVector` is used to store arrays of values for use in PySpark.  `DenseVector` actually stores values in a [NumPy array](http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html) and delegates calculations to that object.  You can create a new `DenseVector` using `DenseVector()` and passing in an NumPy array or a Python list.\n\n`DenseVector` implements several functions, such as `DenseVector.dot()` and `DenseVector.norm()`.\n\nNote that `DenseVector` stores all values as `np.float64`, so even if you pass in a NumPy array of integers, the resulting `DenseVector` will contain floating-point numbers. Also, `DenseVector` objects exist locally and are not inherently distributed.  `DenseVector` objects can be used in the distributed setting by including them in `RDDs` or `DataFrames`.\n\nYou can create a dense vector by using the [Vectors](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors) object and calling `Vectors.dense`.  The `Vectors` object also contains a method for creating `SparseVectors`."],"metadata":{}},{"cell_type":"code","source":["# Create a DenseVector using Vectors\ndenseVector = Vectors.dense([1, 2, 3])\n\nprint 'type(denseVector): {0}'.format(type(denseVector))\nprint '\\ndenseVector: {0}'.format(denseVector)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["** Dot product **\n\nWe can calculate the dot product of two vectors, or a vector and itself, by using `DenseVector.dot()`.  Note that the dot product is equivalent to performing element-wise multiplication and then summing the result.\n\nBelow, you'll find the calculation for the dot product of two vectors, where each vector has length \\\\( n \\\\):\n\n\\\\[ w \\cdot x = \\sum_{i=1}^n w_i x_i \\\\]\n\nNote that you may also see \\\\( w \\cdot x \\\\) represented as \\\\( w^\\top x \\\\)"],"metadata":{}},{"cell_type":"code","source":["denseVector.dot(denseVector)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["** Norm **\n\nWe can calculate the norm of a vector using `Vectors.norm`.  The norm calculation is:\n\n  \\\\[ ||x|| _p = \\bigg( \\sum_i^n |x_i|^p \\bigg)^{1/p} \\\\]\n\n\n\nSometimes we'll want to normalize our features before training a model.  Later on we'll use the `ml` library to perform this normalization using a transformer."],"metadata":{}},{"cell_type":"code","source":["Vectors.norm(denseVector, 2)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["In Python, `DenseVector` operations are delegated to an underlying NumPy array so we can perform multiplication, addition, division, etc."],"metadata":{}},{"cell_type":"code","source":["denseVector * denseVector"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["5 + denseVector"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Sometimes we'll want to treat a vector as an array.  We can convert both sparse and dense vectors to arrays by calling the `toArray` method on the vector."],"metadata":{}},{"cell_type":"code","source":["denseArray = denseVector.toArray()\nprint denseArray"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["print 'type(denseArray): {0}'.format(type(denseArray))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["#### SparseVector"],"metadata":{}},{"cell_type":"code","source":["help(Vectors.sparse)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Let's create a `SparseVector` using [Vectors.sparse](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.Vectors.sparse)."],"metadata":{}},{"cell_type":"code","source":["sparseVector = Vectors.sparse(10, [2, 7], [1.0, 5.0])\nprint 'type(sparseVector): {0}'.format(type(sparseVector))\nprint '\\nsparseVector: {0}'.format(sparseVector)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Let's take a look at what fields and methods are available with a `SparseVector`.  Here are links to the [Python](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector) and [Scala](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.mllib.linalg.SparseVector) APIs for `SparseVector`."],"metadata":{}},{"cell_type":"code","source":["help(SparseVector)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["dir(SparseVector)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["dir(sparseVector)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Show the difference between the two\nset(dir(sparseVector)) - set(dir(SparseVector))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# inspect is a handy tool for seeing the Python source code\nimport inspect\nprint inspect.getsource(SparseVector)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["print 'sparseVector.size: {0}'.format(sparseVector.size)\nprint 'type(sparseVector.size):{0}'.format(type(sparseVector.size))\n\nprint '\\nsparseVector.indices: {0}'.format(sparseVector.indices)\nprint 'type(sparseVector.indices):{0}'.format(type(sparseVector.indices))\nprint 'type(sparseVector.indices[0]):{0}'.format(type(sparseVector.indices[0]))\n\nprint '\\nsparseVector.values: {0}'.format(sparseVector.values)\nprint 'type(sparseVector.values):{0}'.format(type(sparseVector.values))\nprint 'type(sparseVector.values[0]):{0}'.format(type(sparseVector.values[0]))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Don't try to set these values directly.  If you use the wrong type, hard-to-debug errors will occur when Spark attempts to use the `SparseVector`. Create a new `SparseVector` using `Vectors.sparse`"],"metadata":{}},{"cell_type":"code","source":["set(dir(DenseVector)) - set(dir(SparseVector))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["denseVector + denseVector"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["try:\n    sparseVector + sparseVector\nexcept TypeError as e:\n    print e"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["sparseVector.dot(sparseVector)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["sparseVector.norm(2)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["#### LabeledPoint"],"metadata":{}},{"cell_type":"markdown","source":["In MLlib, labeled training instances are stored using the [LabeledPoint](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) object.  Note that the features and label for a `LabeledPoint` are stored in the `features` and `label` attribute of the object."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.regression import LabeledPoint\nhelp(LabeledPoint)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["labeledPoint = LabeledPoint(1992, [3.0, 5.5, 10.0])\nprint 'labeledPoint: {0}'.format(labeledPoint)\n\nprint '\\nlabeledPoint.features: {0}'.format(labeledPoint.features)\n# Notice that feaures are being stored as a DenseVector\nprint 'type(labeledPoint.features): {0}'.format(type(labeledPoint.features))\n\nprint '\\nlabeledPoint.label: {0}'.format(labeledPoint.label)\nprint 'type(labeledPoint.label): {0}'.format(type(labeledPoint.label))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# View the differences between the class and an instantiated instance\nset(dir(labeledPoint)) - set(dir(LabeledPoint))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["labeledPointSparse = LabeledPoint(1992, Vectors.sparse(10, {0: 3.0, 1:5.5, 2: 10.0}))\nprint 'labeledPointSparse: {0}'.format(labeledPointSparse)\n\nprint '\\nlabeledPoint.featuresSparse: {0}'.format(labeledPointSparse.features)\nprint 'type(labeledPointSparse.features): {0}'.format(type(labeledPointSparse.features))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["#### Rating"],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.recommendation import Rating\nhelp(Rating)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["When performing collaborative filtering we aren't working with vectors or labeled points, so we need another type of object to capture the relationship between users, products, and ratings.  This is represented by a `Rating` which can be found in the [Python](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.Rating) and [Scala](https://spark.apache.org/docs/1.5.0/api/scala/index.html#org.apache.spark.mllib.recommendation.Rating) APIs."],"metadata":{}},{"cell_type":"code","source":["print inspect.getsource(Rating)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["rating = Rating(4, 10, 2.0)\n\nprint 'rating: {0}'.format(rating)\n# Note that we can pull out the fields using a dot notation or indexing\nprint rating.user, rating[0]"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["#### DataFrames\n\nWhen using Spark's ML library rather than MLlib you'll be working with `DataFrames` instead of `RDDs`.  In this section we'll show how you can create a `DataFrame` using MLlib datatypes."],"metadata":{}},{"cell_type":"markdown","source":["Above we saw that `Rating` is a `namedtuple`.  `namedtuples` are useful when working with `DataFrames`.  We'll explore how they work below and use them to create a `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["from collections import namedtuple\nhelp(namedtuple)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["Address = namedtuple('Address', ['city', 'state'])\naddress = Address('Boulder', 'CO')\nprint 'address: {0}'.format(address)\n\nprint '\\naddress.city: {0}'.format(address.city)\nprint 'address[0]: {0}'.format(address[0])\n\nprint '\\naddress.State: {0}'.format(address.state)\nprint 'address[1]: {0}'.format(address[1])"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["display(sqlContext.createDataFrame([Address('Boulder', 'CO'), Address('New York', 'NY')]))"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["Let's create a `DataFrame` with a couple of rows where the first column is the label and the second is the features."],"metadata":{}},{"cell_type":"code","source":["LabelAndFeatures = namedtuple('LabelAndFeatures', ['label', 'features'])\nrow1 = LabelAndFeatures(10, Vectors.dense([1.0, 2.0]))\nrow2 = LabelAndFeatures(20, Vectors.dense([1.5, 2.2]))\n\ndf = sqlContext.createDataFrame([row1, row2])\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["#### Exercises"],"metadata":{}},{"cell_type":"markdown","source":["Create a `DenseVector` with the values 1.5, 2.5, 3.0 (in that order)."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndenseVec = <FILL IN>"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["# TEST\nfrom test_helper import Test\nTest.assertEquals(denseVec, DenseVector([1.5, 2.5, 3.0]), 'incorrect value for denseVec')"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["Create a `LabeledPoint` with a label equal to 10.0 and features equal to `denseVec`"],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nlabeledP = <FILL IN>"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["# TEST\nTest.assertEquals(str(labeledP), '(10.0,[1.5,2.5,3.0])', 'incorrect value for labeledP')"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["** Challenge Question [Intentionally Hard]**\n\nCreate a `udf` that pulls the first element out of a column that contains `DenseVectors`."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n# You'll need to include some imports as well\n\n# If you get a pickle exception try casting your element with float()\nfirstElement = <FILL IN>\n\ndf2 = df.select(firstElement('features').alias('first'))\ndf2.show()"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["# TEST\nTest.assertEquals(df2.rdd.map(lambda r: r[0]).collect(), [1.0, 1.5], 'incorrect implementation of firstElement')"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":66}],"metadata":{"name":"mllib-datatypes-master","notebookId":945777816841833},"nbformat":4,"nbformat_minor":0}
